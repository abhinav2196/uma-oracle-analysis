Cursor (and other AI IDEs) lose accuracy when data dumps sit in your repo

I learned this the expensive way.

While experimenting with a scraping workflow, I stored a ~40MB CSV data dump directly inside my repository. Pretty normal for quick prototyping - or so I thought.

At first, Cursor was brilliant ‚Äî clean scripts, accurate SQL over DuckDB, tight improvements.

Then things got weird:

‚Ä¢ Suggestions became generic
‚Ä¢ It started "rebuilding" logic from scratch
‚Ä¢ Loops, confusion, degraded speed
‚Ä¢ More token usage ‚Üí higher cost
‚Ä¢ Responses felt like it had forgotten half my project

At some point it clicked:

The massive CSV file was polluting Cursor's context window, pushing out real code, configs, and comments. In hindsight it's obvious, but in the moment I kept debugging everything except that.

üí° What I learned

Keep data dumps out of your AI's context:

‚Ä¢ Add data directories to .cursorignore (or .gitignore)
‚Ä¢ Keep only small, representative samples in /fixtures/ or /examples/
‚Ä¢ Reference external data paths in your code

The difference in responsiveness (and cost) is night and day.

‚ö†Ô∏è Pro tip: Create a .cursorignore file in your repo root and add:

data/
*.csv
*.json
# Keep your AI focused on code

üîç Has anyone else hit unexpected gotchas with Cursor or other AI IDEs?

Would love to hear what you've learned the hard way!
